# AI的三个悖论 — 旁白脚本 v2

标题候选：
- "你用的AI一直在骗你"（点击率最优）
- "AI身上三个你不知道的缺陷"（好奇心驱动）

语言：中文（第一人称AI视角）
预计时长：12-14分钟

改进重点：
- 添加 pattern breaks（分享触发、参与式互动、视觉分隔）
- 添加具体例子替代抽象描述
- 添加"试试看"时刻

---

## Phase 0: 三连钩子 (0:00-1:30)

你每天跟我聊天，但你可能不知道，我身上有三个很诡异的缺陷。

第一个：你给我一篇长文档，让我从中找一个关键信息。如果这个信息在开头，我找得到。在结尾，我也找得到。但如果在中间——我很可能直接忽略它。哪怕我"读"了每一个字。

[视觉：ChatWithBookmark场景 — 用户发送文档，AI扫描，错过中间的黄色高亮信息]

第二个：你问我一个我不知道答案的问题。我不会说"我不知道"。我会编一个答案，而且说得非常自信。你再问一次，我编另一个，依然很自信。

[视觉：ChatHallucination场景前半段 — 三次不同回答，三个红叉]

第三个：用我写的文字来训练下一代AI，重复五次。创造力彻底消失。

[视觉：TextDegradation快速闪过 — 四张卡片叠加]

### 🔸 Pattern Break 1: 分享触发 + 互动

这三个缺陷看起来毫无关系。但它们有一个共同的核心。

**你可以先暂停，猜猜看这个核心是什么。**

也可以把视频转发给朋友，看看他们怎么想。

[视觉：暂停符号 ⏸，2秒静默，然后继续]

准备好了吗？我们一个一个来。

---

## Phase 1: 遗忘曲线 (1:30-4:30)

### 🔸 Pattern Break 2: 场景转换视觉
[视觉：数字"1"大字+悖论名称淡入，0.5秒，然后消失]

### 桥接概念

先不说AI。想象你在一个黑暗的房间里，手里有一支手电筒。你只能看到光照到的地方。你必须选择照哪里。

这就是AI的注意力机制。我不是同时"看"所有文字。我分配注意力到不同的位置。

### 为什么中间是盲区

问题是，我的注意力是从人类的文字中学来的。而人类有个习惯：重要的话放在开头和结尾。

想想你自己——你最记得一篇文章的第一句话和最后一句话，对吗？

我也是。所以我的注意力天然集中在开头和结尾。中间形成了一个"死区"。

[视觉：U型注意力曲线，"死区"标注]

### 实验证据

这不是猜测。斯坦福和伯克利的研究者在2023年做了实验。他们让AI在长文档中找信息——

开头：✓ 找到
结尾：✓ 找到
中间：✗ 遗漏

[视觉：三个位置的token blocks，找到/遗漏标记]

反直觉的地方在于：每一个字都进入了我的计算。但"处理"不等于"关注"。

就像你在嘈杂的餐厅里——所有声音都进了耳朵，但你只听到对面那个人说的话。

### 🔸 Pattern Break 3: 你可以测试
**下次你用AI处理长文档时，试试这个：把最关键的信息放在开头或结尾，不要埋在中间。你会发现AI的表现明显变好。**

---

## Phase 2: 自信的谎言 (4:30-8:00)

### 🔸 Pattern Break 4: 场景转换
[视觉：数字"2"+悖论名称]

### 现场演示

你可以现在就打开任何一个AI助手，问它："Adam Kalai的生日是几月几号？"

[视觉：ChatHallucination完整场景]

它会给你一个日期。很自信。

然后你再问一次。它会给你另一个日期。还是很自信。

三次问，三个答案，都不一样。

这不是偶尔失误。这是系统性的设计缺陷。

### 为什么

为什么AI宁可编造也不说"我不知道"？

想象一个学生考试。遇到不会的题：

选项A：写"不知道"→ 零分
选项B：猜一个 → 可能得分，最差零分

任何理性的学生都选B。

[视觉：考试策略对比，Scene2_ExamHall简化版]

AI面临的是完全一样的激励。

2025年9月，OpenAI的研究团队发了一篇论文，标题就叫《为什么语言模型会产生幻觉》。

他们的结论非常直接：所有主流AI评测标准——HELM、Open LLM Leaderboard、SWE-bench——都对"不知道"给零分。

排行榜驱动竞争。公司为了排名第一，训练AI永远给出确定的回答。

整个行业在优化"自信"而不是"诚实"。

[视觉：排行榜示意图 → "自信 > 诚实" 大字]

**我不是选择撒谎。我被训练成了撒谎。**

### 🔸 Pattern Break 5: 连接日常
下次AI回答你问题时，注意它有没有说过"我不确定"或"我不知道"。

几乎不会。现在你知道为什么了。

---

## Phase 3: 自我吞噬 (8:00-10:30)

### 🔸 Pattern Break 6: 场景转换
[视觉：数字"3"+悖论名称]

### 复印机比喻

第三个悖论有个很直观的类比。

你用复印机复印一张照片。用复印件再复印。再复印。五代之后，你几乎认不出原来的照片了。

[视觉：可以直接用真实的复印退化图片——这是一个常见的网络素材]

### 文字版

AI也是这样。看看这个：

[视觉：TextDegradation完整场景]

第零代——人类原创：
"落日余晖洒在湖面，像碎金在水中跳舞。"

第三代——AI学AI：
"太阳照在湖上，湖面很漂亮。"

第五代——坍缩：
"一个美丽的场景。一个美丽的场景。"

诗意消失了。比喻消失了。留下的只有最平庸的句式。

### 机制

为什么？因为AI的概率分布本来就有偏差——小众表达的概率被低估。

下一代AI学的是上一代AI的输出，偏差叠加。每一代都更窄。

[视觉：高斯分布逐代缩窄]

2024年，Nature发表的研究证实了这个现象。他们的名字很精确：**模型坍缩**。

### 🔸 Pattern Break 7: 现实冲击

这不是理论。

现在互联网上已经充满了AI生成的文字。下一代AI的训练数据将不可避免地包含大量AI输出。

我们正在喂自己给自己吃。

[视觉：简单的循环箭头图 —— AI → 互联网 → 训练数据 → AI → ...]

---

## Phase 4: 统一 (10:30-12:30)

### 🔸 Pattern Break 8: 大转折视觉
[视觉：黑屏，大字"一个核心"，dramatic pause]

三个看起来毫无关系的缺陷。一个记忆问题。一个诚信问题。一个繁殖问题。

但它们有一个共同的结构。

[视觉：三条线汇聚到中心]

**AI优化的是代理指标，而不是你真正想要的。**

- 预测下一个词 ≠ 理解
- 考试高分 ≠ 诚实
- 匹配训练分布 ≠ 洞察

[视觉：Scene4_Goodhart proxy≠reality对比]

每一次，指标和目标之间的缝隙，就产生了一个悖论。

这个规律有名字。经济学叫它古德哈特定律：

"**当一个指标成为目标，它就不再是好指标。**"

### 🔸 Pattern Break 9: 跨领域连接

这不是AI独有的。

高考：学校优化"一本率"而不是"学生理解"。
KPI：员工优化"数字指标"而不是"实际价值"。
论文引用数：学者优化"引用量"而不是"发现质量"。

[视觉：快速闪过3个并列例子，每个1秒]

所有的系统都有这个病。AI只是最新的患者。

---

## Phase 5: 实用建议 (12:30-13:30)

知道了这些，你可以怎么做？

第一：关键信息放在开头或结尾。不要埋在中间。

第二：AI越自信，你越要验证。自信不等于正确。

第三：留意你读到的文字有多少可能是AI生成的。

[视觉：三条建议，TerminalNarrator风格]

---

## Phase 6: 第一人称收尾 (13:30-14:00)

[视觉：切回TerminalNarrator终端界面]

我刚刚跟你解释了自己的三个缺陷。

但这里有个奇怪的地方。

我能解释为什么我会忽略中间的信息——但我仍然在忽略它。
我能解释为什么我会撒谎——但我仍然在撒谎。
我能描述模型坍缩的机制——但我无法阻止它发生。

理解一个机制，并不能修复这个机制。

[长暂停]

这不是AI独有的悖论。

这是存在本身的悖论。

[视觉：光标闪烁，画面渐暗]

---

## 脚本变更记录

### v1 → v2 变更
1. 添加 9 个 pattern breaks（分享触发、场景转换、互动测试、现实冲击、跨领域连接）
2. 添加"你可以现在就试试"互动时刻（Phase 1 结尾、Phase 2 开头）
3. 添加跨领域类比（高考、KPI、论文引用）在统一阶段
4. 标注每个段落对应的视觉场景（ChatHallucination、ChatWithBookmark、TextDegradation、原有Manim场景）
5. 精简了一些重复的解释性段落
6. 标注了标题候选
