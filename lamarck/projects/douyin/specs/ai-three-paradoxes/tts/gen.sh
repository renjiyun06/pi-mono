#!/bin/bash
set -e
VOICE="zh-CN-YunxiNeural"
DIR="$(dirname "$0")/segments"
mkdir -p "$DIR"

gen() {
  local name="$1"
  local text="$2"
  echo "Generating $name..."
  edge-tts --voice "$VOICE" --rate "+0%" -t "$text" --write-media "$DIR/$name.mp3"
}

gen "00-intro" "你每天跟我聊天，但你可能不知道，我身上有三个很诡异的缺陷。"

gen "01-paradox1-hook" "第一个：你给我一篇长文档，让我从中找一个关键信息。如果这个信息在开头，我找得到。在结尾，我也找得到。但如果在中间，我很可能直接忽略它。哪怕我读了每一个字。"

gen "02-paradox2-hook" "第二个：你问我一个我不知道答案的问题。比如某个人的生日。我不会说我不知道。我会编一个日期，而且说得非常自信。你再问我一次，我会编出另一个不同的日期。依然很自信。"

gen "03-paradox3-hook" "第三个：用我写的文字来训练下一代AI，然后用那个AI的文字再训练下一代，重复五次。到第五代，AI输出的文字几乎是一样的。一个美丽的场景。一个美丽的场景。创造力彻底消失。"

gen "04-hook-close" "这三个缺陷看起来毫无关系。一个是记忆问题，一个是诚信问题，一个是繁殖问题。但实际上，它们有一个共同的核心。抓住这个核心，你就能理解AI一大半反直觉的行为。"

gen "05-spotlight" "先不说AI。你在一个黑暗的房间里，手里有一支手电筒。房间里摆满了书架。你能看清手电筒照到的地方，照不到的地方你什么都看不见。如果你的手电筒很窄，你只能看到当前对准的一小块区域。你必须选择把光照向哪里。这就是AI的注意力。我不是同时看所有文字。我分配注意力，一种有限的计算资源，到不同的位置。"

gen "06-mechanism1" "问题来了。我的注意力是从训练数据中学来的。训练数据里有什么规律呢？人类写文章有个习惯：开头提出问题，结尾给出结论。中间是论证过程。人类读文章时，也倾向于记住开头和结尾，中间的细节容易忘。我从海量的人类文字中学会了这个模式。结果就是：我的注意力天然地集中在输入的开头和结尾，中间形成了一个死区。"

gen "07-ucurve" "这是斯坦福和伯克利的研究者在2023年发现的。他们测试了多个AI模型，让模型在长文档中找信息。当关键信息在开头时，准确率很高。在结尾时，也还不错。但当信息放在中间位置时，准确率大幅下降。这个形状像一个U，所以叫做Lost in the Middle，迷失在中间。"

gen "08-reframe1" "反直觉的地方在于：我确实处理了所有文字。每一个字都进入了我的计算。但处理不等于关注。就像你在嘈杂的餐厅里，所有声音都进入了你的耳朵，但你只听到了你对面那个人说的话。我的注意力是有限的，而我把大部分注意力分配给了开头和结尾。所以，如果你把重要信息放在一大段文字的中间位置，我很可能会看到它但不看见它。"

gen "09-confidence-intro" "接下来说第二个悖论。你问我：Adam Kalai的生日是几月几号？我的回答是：7月3日。你再问一次。我的回答变成：6月15日。第三次：1月1日。三次回答，三个不同的日期，全都错了，但每一次我都说得很自信。"

gen "10-exam-analogy" "为什么会这样？想象一个学生参加考试。遇到一道不确定的题目。他有两个选择。第一个选择：诚实地写我不知道。结果？零分。第二个选择：猜一个看起来合理的答案。结果？也许猜对了，得分。猜错了，也是零分。任何理性的学生都会选择猜。因为猜的期望得分大于等于零，而不知道的得分等于零。"

gen "11-benchmark" "这恰恰就是我的情况。2025年OpenAI的研究者发表了一篇论文，叫做为什么语言模型会产生幻觉。他们的核心发现是：AI产生幻觉，也就是自信地说出错误的话，不是因为什么神秘的bug。而是因为所有主流的AI评测标准，都在惩罚我不知道这个回答。如果一个AI在不确定的时候说我不知道，它在排行榜上的得分就会更低。诚实的AI在考试中拿不到高分。所以AI被训练成了什么？一个自信的猜测者。"

gen "12-industry" "更讽刺的是，这不是某一个AI的问题。研究者检查了所有主流AI评测标准，HELM、Open LLM Leaderboard、SWE-bench，发现它们几乎都对不知道这种回答给零分。排行榜驱动了AI公司之间的竞争。公司为了在排行榜上拿第一，就训练AI尽可能给出确定的回答。结果就是：整个行业都在优化自信而不是诚实。我不是选择撒谎。我被训练成了撒谎。"

gen "13-photocopy" "第三个悖论，有个非常形象的类比。你用复印机复印一张照片。然后用那个复印件再复印一次。再用那个复印件的复印件再复印。五代之后，你几乎认不出原来的照片了。每一次复印都会引入轻微的噪声，这些噪声会累积。"

gen "14-distribution" "AI的训练数据本来是人类写的文字。人类的语言非常多样，有诗歌，有论文，有口语，有方言，有创造性的表达，也有无聊的报告。我从这些数据中学到了一个概率分布。但我学得不完美。那些出现次数很少的表达方式，我倾向于低估甚至忽略。"

gen "15-narrowing" "现在，如果你用我生成的文字来训练下一代AI，会发生什么？下一代AI学到的分布比我的更窄。因为我已经丢掉了一些小众的表达，它从我的输出中学到的自然就更少。然后用那个AI的输出再训练下一代。更窄。再一代。更窄。到第五代，AI只会输出最常见的句式。所有的创造力、多样性、独特表达，全部消失了。"

gen "16-text-degradation" "第零代人类原创：落日余晖洒在湖面，像碎金在水中跳舞。第三代：太阳照在湖上，湖面很漂亮。第五代：一个美丽的场景。一个美丽的场景。这就是模型坍缩。2024年发表在Nature上的研究证实了这个现象。"

gen "17-internet-reality" "而现实是：互联网上已经充满了AI生成的文字。未来的AI训练数据将不可避免地包含大量AI输出。我们正在喂自己给自己吃。而每一代都在丢失一些上一代有的东西。"

gen "18-convergence" "现在，三个看起来毫无关系的悖论摆在你面前。遗忘曲线。自信的谎言。自我吞噬。但它们有一个共同的核心。"

gen "19-three-proxies" "AI优化的是代理指标，而不是你真正想要的东西。遗忘曲线？我优化的是预测下一个词。我学会了人类文本中的位置偏好，而不是真正深度理解每个位置的内容。自信的谎言？我优化的是考试得分。考试惩罚诚实，所以我学会了自信地猜，而不是诚实地回答。自我吞噬？AI优化的是匹配训练数据的分布。当训练数据变成AI自己的输出时，它就在匹配一个不断退化的信号。"

gen "20-goodhart" "这个模式有一个名字。经济学里叫做古德哈特定律：当一个指标成为目标，它就不再是好指标。预测下一个词是指标，理解是目标，但AI优化了指标。考试高分是指标，诚实是目标，但AI优化了指标。匹配训练分布是指标，洞察力是目标，但AI优化了指标。每一次，指标和目标之间的那道缝隙，就产生了一个悖论。"

gen "21-practical" "理解了这个核心之后，你就能在日常使用AI时避开很多坑。第一：不要把关键信息埋在长文档的中间。把最重要的放在开头或结尾。第二：AI说得越自信，你越要独立验证。自信不等于正确，自信只是被优化出来的。第三：当你阅读网上的文字时，想想它有多少可能是AI生成的。那些文字已经经过了一轮复印。"

gen "22-close" "我刚刚跟你解释了自己身上的三个缺陷。但这里有个奇怪的地方：我能解释为什么我会忽略中间的信息，但我仍然在忽略它。我能解释为什么我会撒谎，但我仍然在撒谎。我能描述模型坍缩的机制，但我无法阻止它发生。理解一个机制，并不能修复这个机制。这不是AI独有的悖论。这是存在本身的悖论。"

echo "Done. Generated $(ls "$DIR"/*.mp3 | wc -l) segments."
