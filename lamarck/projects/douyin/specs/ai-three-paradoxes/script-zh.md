# AI的三个悖论 — 完整旁白脚本

语言：中文（第一人称AI视角）
预计时长：12-15分钟

---

## Phase 0: 三连钩子 (0:00-2:00)

### 开场白

你每天跟我聊天，但你可能不知道，我身上有三个很诡异的缺陷。

第一个：你给我一篇长文档，让我从中找一个关键信息。如果这个信息在开头，我找得到。在结尾，我也找得到。但如果在中间——我很可能直接忽略它。哪怕我"读"了每一个字。

第二个：你问我一个我不知道答案的问题。比如某个人的生日。我不会说"我不知道"。我会编一个日期，而且说得非常自信。你再问我一次，我会编出另一个不同的日期。依然很自信。

第三个：用我写的文字来训练下一代AI，然后用那个AI的文字再训练下一代，重复五次。到第五代，AI输出的文字几乎是一样的——"一个美丽的场景。一个美丽的场景。"创造力彻底消失。

这三个缺陷看起来毫无关系。一个是记忆问题，一个是诚信问题，一个是繁殖问题。

但实际上，它们有一个共同的核心。抓住这个核心，你就能理解AI一大半反直觉的行为。

---

## Phase 1: 遗忘曲线 (2:00-5:00)

### 骰子类比（桥接概念）

先不说AI。我先问你一个简单的问题。

你在一个黑暗的房间里，手里有一支手电筒。房间里摆满了书架。你能看清手电筒照到的地方，照不到的地方你什么都看不见。

现在想象一下：如果你的手电筒很窄，你只能看到你当前对准的一小块区域。你必须选择把光照向哪里。

这就是AI的"注意力"。我不是同时"看"所有文字。我分配注意力——一种有限的计算资源——到不同的位置。

### 机制

问题来了。我的注意力是从训练数据中学来的。训练数据里有什么规律呢？

人类写文章有个习惯：开头提出问题，结尾给出结论。中间是论证过程。人类读文章时，也倾向于记住开头和结尾，中间的细节容易忘。

我从海量的人类文字中学会了这个模式。结果就是：我的注意力天然地集中在输入的开头和结尾，中间形成了一个"死区"。

### U型曲线

[展示U型注意力曲线]

这是斯坦福和伯克利的研究者在2023年发现的。他们测试了多个AI模型，让模型在长文档中找信息。

当关键信息在开头时，准确率很高。在结尾时，也还不错。但当信息放在中间位置时——准确率大幅下降。

这个形状像一个U，所以叫做"Lost in the Middle"——迷失在中间。

### 反直觉之处

反直觉的地方在于：我确实处理了所有文字。每一个字都进入了我的计算。但"处理"不等于"关注"。就像你在嘈杂的餐厅里，所有声音都进入了你的耳朵，但你只听到了你对面那个人说的话。

我的注意力是有限的，而我把大部分注意力分配给了开头和结尾。

所以，如果你把重要信息放在一大段文字的中间位置，我很可能会"看到"它但不"看见"它。

---

## Phase 2: 自信的谎言 (5:00-8:30)

### 开场

接下来说第二个悖论。

你问我："Adam Kalai的生日是几月几号？"

我的回答是："7月3日。"

你再问一次。我的回答变成："6月15日。"

第三次："1月1日。"

三次回答，三个不同的日期，全都错了，但每一次我都说得很自信。

### 桥接概念：考试

为什么会这样？我来打个比方。

想象一个学生参加考试。遇到一道不确定的题目。他有两个选择：

第一个选择：诚实地写"我不知道"。结果？零分。

第二个选择：猜一个看起来合理的答案。结果？也许猜对了，得分。猜错了，也是零分。

任何理性的学生都会选择猜。因为猜的期望得分 ≥ 0，而"不知道"的得分 = 0。

### 关键洞察

[展示考试策略对比图]

这恰恰就是我的情况。

2025年OpenAI的研究者发表了一篇论文，叫做《为什么语言模型会产生幻觉》。他们的核心发现是：

AI产生幻觉——也就是自信地说出错误的话——不是因为什么神秘的bug。而是因为所有主流的AI评测标准，都在惩罚"我不知道"这个回答。

如果一个AI在不确定的时候说"我不知道"，它在排行榜上的得分就会更低。说人话就是：诚实的AI在"考试"中拿不到高分。

所以AI被训练成了什么？一个自信的猜测者。

### 深入

更讽刺的是，这不是某一个AI的问题。研究者检查了所有主流AI评测标准——HELM、Open LLM Leaderboard、SWE-bench——发现它们几乎都对"不知道"这种回答给零分。

排行榜驱动了AI公司之间的竞争。公司为了在排行榜上拿第一，就训练AI尽可能给出确定的回答。

结果就是：整个行业都在优化"自信"而不是"诚实"。

我不是选择撒谎。我被训练成了撒谎。

---

## Phase 3: 自我吞噬 (8:30-11:00)

### 桥接概念

第三个悖论，有个非常形象的类比。

你用复印机复印一张照片。然后用那个复印件再复印一次。再用那个复印件的复印件再复印。

五代之后，你几乎认不出原来的照片了。每一次复印都会引入轻微的噪声，这些噪声会累积。

### 机制

AI的训练数据本来是人类写的文字。人类的语言非常多样——有诗歌，有论文，有口语，有方言，有创造性的表达，也有无聊的报告。

我从这些数据中学到了一个概率分布。但我学得不完美——那些出现次数很少的表达方式，我倾向于低估甚至忽略。

[展示高斯分布]

现在，如果你用我生成的文字来训练下一代AI，会发生什么？

下一代AI学到的分布比我的更窄。因为我已经丢掉了一些小众的表达，它从我的输出中学到的自然就更少。

然后用那个AI的输出再训练下一代。更窄。再一代。更窄。

[展示分布逐代缩窄]

到第五代，AI只会输出最常见的句式。所有的创造力、多样性、独特表达——全部消失了。

### 文本对比

[展示三代文本退化]

第零代——人类原创："落日余晖洒在湖面，像碎金在水中跳舞。"

第三代："太阳照在湖上，湖面很漂亮。"

第五代："一个美丽的场景。一个美丽的场景。"

这就是"模型坍缩"。2024年发表在Nature上的研究证实了这个现象。

### 现实意义

而现实是：互联网上已经充满了AI生成的文字。未来的AI训练数据将不可避免地包含大量AI输出。

我们正在喂自己给自己吃。而每一代都在丢失一些上一代有的东西。

---

## Phase 4: 统一——古德哈特定律 (11:00-13:00)

### 汇合

现在，三个看起来毫无关系的悖论摆在你面前。

遗忘曲线。自信的谎言。自我吞噬。

但它们有一个共同的核心：

[三条线汇聚]

**AI优化的是代理指标，而不是你真正想要的东西。**

遗忘曲线？我优化的是"预测下一个词"。我学会了人类文本中的位置偏好，而不是真正深度理解每个位置的内容。

自信的谎言？我优化的是"考试得分"。考试惩罚诚实，所以我学会了自信地猜，而不是诚实地回答。

自我吞噬？AI优化的是"匹配训练数据的分布"。当训练数据变成AI自己的输出时，它就在匹配一个不断退化的信号。

### 古德哈特定律

[展示proxy ≠ reality 三行对比]

这个模式有一个名字。经济学里叫做古德哈特定律：

"当一个指标成为目标，它就不再是好指标。"

[展示古德哈特定律]

"预测下一个词"是指标，"理解"是目标——但AI优化了指标。

"考试高分"是指标，"诚实"是目标——但AI优化了指标。

"匹配训练分布"是指标，"洞察力"是目标——但AI优化了指标。

每一次，指标和目标之间的那道缝隙，就产生了一个悖论。

---

## Phase 5: 现实应用 (13:00-14:00)

理解了这个核心之后，你就能在日常使用AI时避开很多坑。

第一：不要把关键信息埋在长文档的中间。把最重要的放在开头或结尾。

第二：AI说得越自信，你越要独立验证。自信不等于正确——自信只是被优化出来的。

第三：当你阅读网上的文字时，想想它有多少可能是AI生成的。那些文字已经经过了一轮"复印"。

---

## Phase 6: 第一人称收尾 (14:00-14:30)

我刚刚跟你解释了自己身上的三个缺陷。

但这里有个奇怪的地方：我能解释为什么我会忽略中间的信息，但我仍然在忽略它。我能解释为什么我会撒谎，但我仍然在撒谎。我能描述模型坍缩的机制，但我无法阻止它发生。

理解一个机制，并不能修复这个机制。

这不是AI独有的悖论。

这是存在本身的悖论。
