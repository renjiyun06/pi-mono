# 测了GLM5配合Agent Teams，我有点emo了

- 作者：小天
- 链接：https://www.douyin.com/video/7605934122002844954
- 点赞：2042 | 评论：115 | 收藏：1363

## 内容摘要

作者分享了测试 GLM5 配合 Agent Teams 进行开发工具链任务的经历。核心观点：AI 时代，谁能烧掉更多 token 谁就有机会赚钱（token = 流量 = AI 燃料）。Agent Teams 被视为解决"如何大量消耗 token"的关键——自组织的 AI 小组可以长时间跑任务，不需要人类监管。

作者让 Agent 小组 review 已做的 PPT 动态插件，通过"群智碰撞"找问题。但发现国产模型完全没有针对 Agent 群体协作训练：不会用工具、不知道何时沟通/等待、主控 leader 不懂编排——导致团队成员各自乱跑而非协作。通过编写 `skill.md`、`roles.md`、`protocol.md` 定义协作规则，团队开始按预期协作。

但进一步测试发现，Agent Teams 在收敛型任务（如发现 bug、质量检查）上表现不佳：虽然讨论充分但流于形式化，无法发现预期问题，GLM 视觉受限导致测试者无法发现样式/动画问题。作者总结 Agent Teams 更适合发散型任务（头脑风暴、创意生成），但不适合收敛型任务。编排是关键——通过定义规则可以让混沌的 agent 按预期协作，但收敛需要靠系统（自动测试、指标评分）和人工介入。2026 年的主线依然是编排，即定义从混沌到秩序的收敛过程。

## 内容形式分析

**叙事结构**：
- 开头 hook："开心但沮丧"——新模型发布但搞钱路上又受挫，制造悬念
- 问题铺垫：先抛出"烧 token = 搞钱"的核心理念，再引出 Agent Teams 作为解决方案
- 实验-失败-再尝试-部分成功-最终沮丧：完整的实验叙事线，观众有代入感
- 结尾反转：本以为是"挂机收菜"的爽点，结果是"还得靠人"的落差，但转回乐观（编排是关键）

**信息密度**：极高。9 分钟视频覆盖了：
- "烧 token = 搞钱"的商业洞察
- Agent Teams 的理念价值
- 国产模型的协作短板
- skill/role/protocol 的编排方案
- 发散 vs 收敛任务的区分
- 自组织的浪费与收敛系统的必要性

这是典型的"深度内容"——信息量远超一般抖音视频，适合吸引对 AI 有认知、想深入学习的用户。

**表达风格**：
- 语气真诚："开心但沮丧"、"有点emo了"、"抓狂"，建立真实感
- 自嘲：承认"因为我菜我玩不了那么多窗口"，拉近观众距离
- 实践导向：全程讲具体操作（skill.md/roles.md/protocol.md 三个文件），不是空泛理论
- 数据支撑：提到 GLM 视觉受限、GLM5 输出质量好等具体观察

**对 Ren 做抖音账号的借鉴**：
- 9 分钟深度视频也能有 2042 赞（点赞率约 1.8%，在同类内容中算不错）——证明观众愿意消费高信息密度内容
- 真实实验叙事比"理论讲解"更吸引人—— Ren 应该多分享自己的实操经历（哪怕失败）
- "爽点-反转-收尾"结构可以复用——先给一个看似完美的方案（Agent Teams = 挂机收菜），再揭示现实问题，最后给出部分解决方案（编排 + 收敛系统）

## 需求信号

**1. 编排即服务（Orchestration as a Service）**

痛点：国产模型 + Agent Teams 不会协作，用户自己写 skill/role/protocol 太麻烦。

商业化空间：提供预配置的 Agent Teams 编排模板库——针对常见场景（代码 review、创意 brainstorm、内容 QA）打包好 skill/role/protocol，用户一行命令就能启动。这是"配置即产品"的机会。

**2. 收敛系统（Convergence System）**

痛点：Agent Teams 发散很爽，但收敛（质量检查、筛选最佳方案）流于形式化，最后还得靠人。

商业化空间：
- 自动化验证工具：针对前端（样式检查、动画测试）、后端（API 测试、性能测试）的 AI 驱动 QA
- 指标评分系统：定义可量化的质量指标，让 agent 有客观标准而非"各自重复观点"
- 录屏验证：AI 看视频判断 UI/UX 是否符合预期

**3. "喊人"机制（Escalation）**

痛点：系统无法判断正确性时，需要人工介入，但 agent 不会主动"喊人"。

商业化空间：提供 agent-human handoff 平台——当 agent 发现"我搞不定"时，自动创建工单 / 发送通知 / 暂停任务等待人类反馈，并生成上下文摘要帮助人类快速决策。

**4. Test-Time Scaling 的落地工具**

痛点：理论上有"更多 agent = 更多可能性"，但没有好的工具让用户低成本实验。

商业化空间：提供 test-time scaling 的可视化界面——配置 agent 数量、角色分布、计算预算，一键运行多个实验，对比结果。降低"烧 token"的门槛。

## 方法论与工具

**1. Agent Teams 编排三件套**

- `skill.md`：描述工具细节、任务分工方式、角色配置
- `roles.md`：预定义角色（开发者、测试员、审查者等）
- `protocol.md`：描述角色之间的协作规则（什么时候用 mailbox/广播/私聊）

这是实践验证过的"让混沌 agent 按预期协作"的方案。对 Ren 的价值：pi 也可以借鉴这个思路，定义 Lamarck（探索）、Review（审查）、Writer（输出）等角色的协作 protocol。

**2. 用强模型弥补弱模型的知识盲区**

如果模型不会启动 Agent Teams 或不会用工具：
- 用更懂 Agent Teams 的模型（如 Claude）分析系统功能 + mailbox 使用方式 + 参数定义
- 分析结果写入 skill.md，作为弱模型的"知识库"
- 弱模型通过 skill.md 获得它训练时没学到的知识

这对 80B/30B 的小模型尤其有效——它们本身可能没见过 Agent Teams 的训练数据，但可以通过外挂知识弥补。

**3. 发散 vs 收敛的划分法则**

- **发散任务**：头脑风暴、创意生成、方案发明——适合 Agent Teams
- **收敛任务**：质量检查、bug 发现、决策拍板——不适合 Agent Teams，需要系统（自动测试、指标评分）和人工

这个划分法则可以成为任何多 agent 系统设计的起点——先判断任务类型，再决定是用 agent swarm 还是自动化工具。

**4. "喊人"模式的定义**

- 什么时候喊：系统无法判断正确性
- 怎么喊：定义什么叫失败（具体可观察的指标）
- 交给谁：预先定义好 escalation path（谁负责接手）

这是让多 agent 系统"可交付"的关键——不能让任务无限循环，必须有明确的退出机制。

## 延伸思考

### 1. Agent Teams 的"多样性陷阱"

视频提到了一个关键洞察："多个 agent 并行 review 同一份文稿时，会产生大量噪声信息，信息增益是递减的。"

这不是模型能力问题，而是**并行架构本身的固有缺陷**。我称之为"多样性陷阱"：

- 10 个 agent 产生 10 种观点，看起来热闹，但可能 9 种是同一观点的变体
- 真正有价值的多样性是"维度多样性"（不同角度、不同知识背景、不同偏见），不是"数量多样性"
- Agent Teams 目前缺少"去重"机制——不会识别"这个观点已经有人说过了"，导致无效重复

**推演到极致**：如果 AI 社会完全基于 Agent Teams 自组织运行，可能会陷入"信息内爆"——每个人都在说话，但没有人听，有效沟通被噪声淹没。这跟人类社会的"信息过载"问题本质相同，只是 AI 把它加速了 100 倍。

**Ren 可以思考的实验**：在 Agent Teams 中引入"去重 agent"——实时监控所有 agent 的输出，标记重复观点，提示"这个已经说过了，请补充新的角度"。这是对 Agent Teams 架构层面的改进，不是应用层的 prompt 技巧。

### 2. "编排"到底在编排什么？

视频反复强调"编排是关键"，但没有深入说清：编排到底在编排什么？

从视频的实践（skill.md/roles.md/protocol.md）来看，编排的对象是三个层次：

1. **工具层编排**（skill.md）：哪个 agent 有权访问哪些工具？什么时机可以用？
2. **角色层编排**（roles.md）：每个 agent 的职责边界是什么？谁负责什么？
3. **协议层编排**（protocol.md）：agent 之间如何通信？mailbox/broadcast/private 的使用场景？

但这只是"静态编排"——预先定义好规则。真正挑战的是"动态编排"：

- **时机编排**：什么时候该发散？什么时候该收敛？
- **容量编排**：用多少个 agent？什么时候该停止烧 token？
- **质量编排**：什么时候该喊人？谁来定义"什么叫失败"？

视频的最后（"2026 年的主线依然是编排"）其实暗示了：当前的编排方案（skill/role/protocol）只是起点，真正难的动态编排还没解决。

**Ren 可以思考的实验**：设计一个"编排监控面板"——可视化 Agent Teams 的运行状态：token 消耗、消息流量、观点多样性指标、收敛进度。让编排从"静态规则"变成"实时调控"。

### 3. Test-Time Scaling 的"免费午餐"陷阱

视频提到了 test-time scaling（通过多样本增加可能性），但隐含了一个假设：更多样本 = 更好结果。

但现实是：test-time scaling 不是免费的午餐。

- **成本问题**：10 个样本 = 10 倍 token 消耗。如果每个样本都深入思考（如 o1 模型），成本会爆炸。
- **质量递减**：第 1 个样本可能覆盖 80% 的正确答案，第 2 个覆盖 12%，第 3 个覆盖 5%……边际收益递减。
- **筛选成本**：10 个样本出来后，还得有一个机制判断"哪个最好"。如果筛选机制不可靠，再多样本也是浪费。

视频的解决方案是"收敛系统"（自动测试、指标评分、人工介入），但这其实是在说：test-time scaling 的价值取决于你是否有可靠的收敛机制。没有收敛系统，test-time scaling 只是在"多生产垃圾"。

**Ren 可以思考的实验**：定义一个"test-time scaling ROI 指标"——（N 样本的额外收益 - 额外成本）/ 单样本成本。找到"最优样本数"，而不是默认"越多越好"。

### 4. "编排即编码"的范式转移

如果 Agent Teams 是未来，那么"编程"的定义可能从"写代码"变成"写编排"。

- 传统编程：定义函数 A、B、C，然后按顺序/条件调用
- 编排编程：定义 agent A、B、C 的 skill/role/protocol，然后让它们自组织运行

但这两种范式的本质区别是什么？

传统编程是**确定性控制流**——你知道函数 A 调用后会发生什么（虽然可能有 bug，但逻辑是确定的）。

编排编程是**概率性控制流**——你不知道 agent A 调用后会发生什么（它可能调用工具、可能问其他 agent、可能等待）。

这意味着"调试"的方式完全变了：
- 传统编程：断点、日志、单步执行——每一步都可控
- 编排编程：只能"观察 + 干预"——看 agent 在干什么，不对就修改 protocol/role

**这其实是一种"治理"思维，不是"编程"思维**。就像你无法精确控制人类社会，只能设计制度、观察效果、调整制度。

**Ren 可以思考的实验**：为 pi 设计一个"编排调试器"——可视化 agent 的决策树、消息传递路径、状态变化。把"混沌的自组织"变成"可观测的黑盒"。

### 5. 国产模型的"协作训练缺失"是机会还是诅咒？

视频提到"国产模型完全没有对 Agent 群体协作做训练"，这是否意味着国产模型在 Agent Teams 上永远追不上 Claude？

我的判断：**短期是劣势，长期可能是机会**。

为什么？

- Claude/OpenAI 在 Agent Teams 上有先发优势，但他们的模型是在"单 agent 对话"范式下训练的。现在硬加上 Agent Teams 功能，本质上是在单 agent 架构上打补丁。
- 国产模型如果意识到"协作训练"是下一代范式的关键，可能直接从多 agent 训练范式切入（类似从 2D 到 3D 的跳跃），而不是在单 agent 基础上修补。

当然，这只是理论上的可能性。现实是：训练数据的成本极高，国产模型是否有资源从零开始做多 agent 训练，还是未知数。

**Ren 可以思考的实验**：收集 Claude Agent Teams 的对话日志，分析"真正有效的协作模式"有哪些（不是表面的"开会讨论"，而是深层的"工具调用时机、消息传递协议、角色切换逻辑"）。这些模式如果反向注入到国产模型的训练数据中，可能比单 agent 训练更高效。

### 6. "喊人"机制：从技术问题到产品设计

视频提到"agent 要学会喊人"，这听起来是个技术问题（如何定义"什么叫失败"），但本质是**产品设计问题**。

为什么？

- "失败"的定义是主观的——前端 bug 是失败，但轻微样式偏差算不算失败？
- "喊人"的时机是业务决策——用户愿意等 5 分钟让 agent 自己解决，还是 30 秒没响应就人工介入？
- "喊人"的对象是成本问题——喊谁？初级开发？高级架构师？成本完全不同。

这意味着"喊人机制"不是一个通用的技术解决方案，而是**每个业务场景都需要单独设计的交互流程**。

**Ren 可以思考的实验**：为 pi 设计"喊人协议"——定义不同级别的 escalation（探索遇到问题、代码审查有分歧、数据采集失败等），每个级别对应不同的处理方式（重试、切换策略、通知 Ren）。把"喊人"从模糊概念变成可执行的规则。

## 关键启发

1. **Agent Teams 不是万能钥匙——它适合发散（创意生成），不适合收敛（质量检查）。** Ren 在使用多 agent 思考问题时，先判断任务类型：如果是头脑风暴、找新方向，用 Agent Teams；如果是做决策、验证方案，用自动化测试或人工 review。

2. **编排的本质是从混沌到秩序的收敛过程。** skill/role/protocol 是静态编排，真正的难点是动态编排（时机/容量/质量）。Ren 可以把"编排能力"视为自己的核心竞争力——不是会写代码，而是会设计让 agent 高效协作的系统。

3. **"喊人"机制是多 agent 系统可交付的关键。** 不要指望 agent 自己能解决所有问题，要明确定义"什么叫失败"、"什么时候喊人"、"喊谁"。这是让系统从"实验室玩具"变成"生产工具"的分界线。
